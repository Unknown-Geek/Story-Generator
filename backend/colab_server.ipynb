{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1hc8G2WY_4P_0Tri-lZ0HmVDdX6MKy5LV?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install only required packages\n",
    "!pip install torch torchvision diffusers transformers flask flask-cors xformers\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.cuda import amp  # Add this import\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "from flask import Flask, request, jsonify, make_response\n",
    "from flask_cors import CORS\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from threading import Thread\n",
    "import time\n",
    "from transformers import set_seed\n",
    "import subprocess\n",
    "import sys\n",
    "import requests  # Add missing requests import\n",
    "\n",
    "# Replace cloudflared installation with localtunnel\n",
    "!npm install -g localtunnel\n",
    "\n",
    "# Configure memory settings\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:512\"\n",
    "torch.backends.cuda.max_memory_split_size = 512 * 1024 * 1024  # 512 MB\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Updated CORS configuration with all necessary headers\n",
    "app = Flask(__name__)\n",
    "CORS(app, resources={\n",
    "    r\"/*\": {\n",
    "        \"origins\": \"*\",  # Allow all origins\n",
    "        \"methods\": [\"GET\", \"POST\", \"OPTIONS\"],\n",
    "        \"allow_headers\": [\"*\"],  # Allow all headers\n",
    "        \"expose_headers\": [\"*\"],\n",
    "        \"max_age\": 3600,\n",
    "        \"supports_credentials\": True\n",
    "    }\n",
    "})\n",
    "\n",
    "@app.after_request\n",
    "def after_request(response):\n",
    "    \"\"\"Add CORS headers to all responses\"\"\"\n",
    "    origin = request.headers.get('Origin', '*')\n",
    "    response.headers.update({\n",
    "        'Access-Control-Allow-Origin': origin,\n",
    "        'Access-Control-Allow-Methods': '*',\n",
    "        'Access-Control-Allow-Headers': '*',\n",
    "        'Access-Control-Allow-Credentials': 'true',\n",
    "        'Access-Control-Max-Age': '3600',\n",
    "        'Access-Control-Expose-Headers': '*'\n",
    "    })\n",
    "    return response\n",
    "\n",
    "# Add model download before pipeline setup\n",
    "def download_model():\n",
    "    import subprocess\n",
    "    repo_name = \"stabilityai-sdxl-turbo-turbo-tiny-green-smashed\"\n",
    "    subprocess.run([\"mkdir\", repo_name])\n",
    "    subprocess.run([\n",
    "        \"huggingface-cli\", \"download\",\n",
    "        f'PrunaAI/{repo_name}',\n",
    "        \"--local-dir\", repo_name,\n",
    "        \"--local-dir-use-symlinks\", \"False\"\n",
    "    ])\n",
    "    return repo_name\n",
    "\n",
    "def get_gpu_memory():\n",
    "    try:\n",
    "        return {\n",
    "            'free': torch.cuda.mem_get_info()[0] // 1024**2,  # Convert to MB\n",
    "            'total': torch.cuda.mem_get_info()[1] // 1024**2,\n",
    "            'used': (torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0]) // 1024**2\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def optimize_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        # Clear memory fragments\n",
    "        current_device = torch.cuda.current_device()\n",
    "        torch.cuda.synchronize(current_device)\n",
    "        \n",
    "        # Force garbage collection of CUDA tensors\n",
    "        for obj in gc.get_objects():\n",
    "            try:\n",
    "                if torch.is_tensor(obj) and obj.device.type == 'cuda':\n",
    "                    del obj\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        # Final cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "def setup_pipeline():\n",
    "    optimize_gpu_memory()\n",
    "    \n",
    "    # Set deterministic behavior\n",
    "    set_seed(42)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    # Load model with optimized settings\n",
    "    pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "        \"stabilityai/sdxl-turbo\",\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "        use_safetensors=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"balanced\"  # Keep simple device mapping\n",
    "    )\n",
    "    \n",
    "    # Configure memory settings\n",
    "    pipeline.enable_xformers_memory_efficient_attention()\n",
    "    \n",
    "    # Put all trainable components in eval mode\n",
    "    for component in [pipeline.text_encoder, pipeline.unet, pipeline.vae]:\n",
    "        if component is not None:\n",
    "            component.eval()\n",
    "            if hasattr(component, 'requires_grad_'):\n",
    "                component.requires_grad_(False)\n",
    "    \n",
    "    # Ensure text encoder is on CPU if needed\n",
    "    if pipeline.text_encoder is not None:\n",
    "        pipeline.text_encoder.to('cpu')\n",
    "    \n",
    "    optimize_gpu_memory()\n",
    "    return pipeline, None\n",
    "\n",
    "def find_free_port(start_port=5001, max_attempts=10):\n",
    "    \"\"\"Find first available port starting from start_port\"\"\"\n",
    "    import socket\n",
    "    for port in range(start_port, start_port + max_attempts):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('', port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            continue\n",
    "    raise RuntimeError(f\"Could not find free port in range {start_port}-{start_port + max_attempts}\")\n",
    "\n",
    "def setup_tunnel(port):\n",
    "    \"\"\"Setup tunnel with improved error handling and fixed subdomain\"\"\"\n",
    "    try:\n",
    "        print(\"\\n=== Setting up LocalTunnel ===\")\n",
    "        \n",
    "        # First, ensure no existing tunnels\n",
    "        os.system('pkill -f lt')\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Generate a unique subdomain based on timestamp\n",
    "        timestamp = datetime.now().strftime('%H%M%S')\n",
    "        subdomain = f\"story-gen-{timestamp}\"\n",
    "        \n",
    "        # Start tunnel with more robust command\n",
    "        command = f'npx localtunnel --port {port} --subdomain {subdomain}'\n",
    "        process = subprocess.Popen(\n",
    "            command,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        \n",
    "        # Wait for tunnel URL with improved error handling\n",
    "        start_time = time.time()\n",
    "        timeout = 30  # seconds\n",
    "        url = None\n",
    "        \n",
    "        while time.time() - start_time < timeout:\n",
    "            output = process.stdout.readline()\n",
    "            if not output and process.poll() is not None:\n",
    "                break\n",
    "                \n",
    "            if output:\n",
    "                print(f\"Tunnel output: {output.strip()}\")\n",
    "                if 'your url is:' in output.lower():\n",
    "                    url = output.split('is: ')[-1].strip()\n",
    "                    break\n",
    "                    \n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        if not url:\n",
    "            raise Exception(\"Failed to get tunnel URL within timeout\")\n",
    "            \n",
    "        # Verify tunnel is working\n",
    "        test_url = f\"{url}/health\"\n",
    "        print(f\"Testing connection to {test_url}\")\n",
    "        \n",
    "        for _ in range(3):  # 3 retries\n",
    "            try:\n",
    "                response = requests.get(test_url, \n",
    "                                     headers={'Accept': 'application/json'},\n",
    "                                     timeout=5,\n",
    "                                     verify=False)  # Ignore SSL verification\n",
    "                if response.ok:\n",
    "                    print(\"✓ Tunnel connection verified\")\n",
    "                    return url\n",
    "            except Exception as e:\n",
    "                print(f\"Retry - Connection test failed: {e}\")\n",
    "                time.sleep(2)\n",
    "                \n",
    "        raise Exception(\"Could not verify tunnel connection\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Tunnel setup failed: {str(e)}\")\n",
    "        if process and process.poll() is None:\n",
    "            process.terminate()\n",
    "        raise\n",
    "\n",
    "def run_server():\n",
    "    \"\"\"Run server with improved error handling\"\"\"\n",
    "    try:\n",
    "        print(\"\\n1. Checking GPU...\")\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"⚠️ WARNING: No GPU detected!\")\n",
    "        else:\n",
    "            print(f\"✓ Found GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"✓ Memory: {get_gpu_memory()}\")\n",
    "        \n",
    "        print(\"\\n2. Starting Flask server...\")\n",
    "        # Find available port\n",
    "        port = find_free_port(5001)\n",
    "        print(f\"Found available port: {port}\")\n",
    "        \n",
    "        # Update the Flask server to be more verbose\n",
    "        def run_flask():\n",
    "            app.run(\n",
    "                host='0.0.0.0',\n",
    "                port=port,\n",
    "                debug=False,\n",
    "                use_reloader=False,\n",
    "                threaded=True\n",
    "            )\n",
    "        \n",
    "        server = Thread(target=run_flask)\n",
    "        server.daemon = True\n",
    "        server.start()\n",
    "        time.sleep(2)  # Give more time to start\n",
    "        print(\"✓ Flask server running\")\n",
    "        \n",
    "        # Test local connection\n",
    "        try:\n",
    "            test_response = requests.get(f'http://localhost:{port}/health', timeout=5)\n",
    "            print(f\"✓ Local health check: {test_response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Local health check failed: {e}\")\n",
    "\n",
    "        print(\"\\n3. Setting up tunnel...\")\n",
    "        tunnel_url = None\n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                # Pass the found port to setup_tunnel\n",
    "                tunnel_url = setup_tunnel(port)\n",
    "                if tunnel_url:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                if attempt < max_attempts - 1:\n",
    "                    print(f\"\\nRetrying tunnel setup ({attempt + 1}/{max_attempts})...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    raise Exception(f\"Failed to establish tunnel after {max_attempts} attempts: {str(e)}\")\n",
    "        \n",
    "        if not tunnel_url:\n",
    "            raise Exception(\"Failed to get tunnel URL\")\n",
    "            \n",
    "        print(\"\\n=== SDXL Server Ready ===\")\n",
    "        print(f\"Server URL: {tunnel_url}\")\n",
    "        print(\"Copy this URL to use in the Story Generator app\")\n",
    "        \n",
    "        # Keep the notebook running\n",
    "        try:\n",
    "            while True:\n",
    "                time.sleep(1)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nShutting down server...\")\n",
    "            os.system('pkill -f lt')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")\n",
    "        os.system('pkill -f lt')\n",
    "        raise\n",
    "\n",
    "@app.route('/health', methods=['GET', 'OPTIONS'])\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint with enhanced error handling\"\"\"\n",
    "    if request.method == 'OPTIONS':\n",
    "        return handle_preflight()\n",
    "\n",
    "    try:\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        if not gpu_available:\n",
    "            return jsonify({\n",
    "                'status': 'unhealthy',\n",
    "                'service': 'sdxl',\n",
    "                'error': 'GPU not available',\n",
    "                'gpu_available': False\n",
    "            }), 503\n",
    "\n",
    "        gpu_info = get_gpu_memory()\n",
    "        if not gpu_info or gpu_info['free'] < 2000:  # Less than 2GB free\n",
    "            return jsonify({\n",
    "                'status': 'unhealthy',\n",
    "                'service': 'sdxl',\n",
    "                'error': 'Insufficient GPU memory',\n",
    "                'gpu_available': True,\n",
    "                'gpu_info': gpu_info\n",
    "            }), 503\n",
    "\n",
    "        response = jsonify({\n",
    "            'status': 'healthy',\n",
    "            'service': 'sdxl',\n",
    "            'gpu_available': True,\n",
    "            'gpu_info': gpu_info,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        # Set all required CORS headers\n",
    "        response.headers.update({\n",
    "            'Access-Control-Allow-Origin': '*',\n",
    "            'Access-Control-Allow-Methods': 'GET, OPTIONS',\n",
    "            'Access-Control-Allow-Headers': '*',\n",
    "            'Content-Type': 'application/json',\n",
    "            'Cache-Control': 'no-cache'\n",
    "        })\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Health check failed: {str(e)}\")\n",
    "        return jsonify({\n",
    "            'status': 'error',\n",
    "            'service': 'sdxl',\n",
    "            'error': str(e)\n",
    "        }), 500\n",
    "\n",
    "def handle_preflight():\n",
    "    \"\"\"Handle CORS preflight requests\"\"\"\n",
    "    response = make_response()\n",
    "    response.headers.update({\n",
    "        'Access-Control-Allow-Origin': '*',\n",
    "        'Access-Control-Allow-Methods': 'GET, OPTIONS',\n",
    "        'Access-Control-Allow-Headers': '*',\n",
    "        'Access-Control-Max-Age': '3600'\n",
    "    })\n",
    "    return response\n",
    "\n",
    "@app.route('/generate_image', methods=['POST', 'OPTIONS'])\n",
    "def generate_image():\n",
    "    if request.method == 'OPTIONS':\n",
    "        response = make_response()\n",
    "        response.headers.update({\n",
    "            'Access-Control-Allow-Origin': '*',\n",
    "            'Access-Control-Allow-Headers': 'Content-Type',\n",
    "            'Access-Control-Allow-Methods': 'POST',\n",
    "            'Access-Control-Max-Age': '3600'\n",
    "        })\n",
    "        return response\n",
    "\n",
    "    try:\n",
    "        # Check GPU memory and cleanup if needed\n",
    "        gpu_mem = get_gpu_memory()\n",
    "        if gpu_mem and gpu_mem['free'] < 4000:  # Need at least 4GB free\n",
    "            optimize_gpu_memory()\n",
    "            if get_gpu_memory()['free'] < 4000:\n",
    "                return jsonify({\n",
    "                    'success': False,\n",
    "                    'error': 'Insufficient GPU memory available'\n",
    "                }), 503\n",
    "\n",
    "        data = request.get_json()\n",
    "        prompt = data.get('prompt', '')\n",
    "        print(f\"Received image generation request with prompt: {prompt}\")  # Debug logging\n",
    "\n",
    "        if not prompt:\n",
    "            return jsonify({'success': False, 'error': 'Prompt is required'}), 400\n",
    "\n",
    "        # Check GPU memory before generation\n",
    "        gpu_mem = get_gpu_memory()\n",
    "        if gpu_mem and gpu_mem['free'] < 2000:  # Less than 2GB free\n",
    "            optimize_gpu_memory()\n",
    "\n",
    "        # Use global pipeline\n",
    "        global pipeline\n",
    "        \n",
    "        # Fixed autocast usage and pipeline check\n",
    "        with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "            with torch.inference_mode():\n",
    "                optimize_gpu_memory()\n",
    "                print(\"Generating image...\") # Debug logging\n",
    "                \n",
    "                if pipeline is None:\n",
    "                    raise ValueError(\"Pipeline not initialized\")\n",
    "                \n",
    "                # Check each component individually\n",
    "                for name, component in [\n",
    "                    ('text_encoder', getattr(pipeline, 'text_encoder', None)),\n",
    "                    ('unet', getattr(pipeline, 'unet', None)),\n",
    "                    ('vae', getattr(pipeline, 'vae', None))\n",
    "                ]:\n",
    "                    if component is not None and hasattr(component, 'device'):\n",
    "                        if name == 'text_encoder' and component.device.type != 'cpu':\n",
    "                            component.to('cpu')\n",
    "                        elif name != 'text_encoder' and component.device.type != 'cuda':\n",
    "                            component.to('cuda')\n",
    "                \n",
    "                image = pipeline(\n",
    "                    prompt=prompt,\n",
    "                    num_inference_steps=1,\n",
    "                    guidance_scale=0.0,\n",
    "                    width=384,\n",
    "                    height=384,\n",
    "                    negative_prompt=\"low quality, blurry, distorted\",\n",
    "                ).images[0]\n",
    "                optimize_gpu_memory()\n",
    "                print(\"Image generated successfully\") # Debug logging\n",
    "\n",
    "        # Add error handling for invalid values\n",
    "        if np.isnan(np.array(image)).any():\n",
    "            raise ValueError(\"Generated image contains invalid values\")\n",
    "\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\", optimize=True, quality=85)\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "        # Cleanup\n",
    "        del image\n",
    "        buffered.close()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Sending response\") # Debug logging\n",
    "        return jsonify({\n",
    "            'success': True,\n",
    "            'image': f'data:image/jpeg;base64,{img_str}'\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {str(e)}\") # Debug logging\n",
    "        optimize_gpu_memory()\n",
    "        torch.cuda.empty_cache()\n",
    "        logger.error(f\"Image generation error: {str(e)}\")\n",
    "        return jsonify({\n",
    "            'success': False,\n",
    "            'error': f\"Generation failed: {str(e)}\"\n",
    "        }), 500\n",
    "\n",
    "@app.route('/verify_connection', methods=['POST'])\n",
    "def verify_connection():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        backend_url = data.get('backend_url')\n",
    "        timestamp = data.get('timestamp')\n",
    "\n",
    "        if not backend_url:\n",
    "            return jsonify({\n",
    "                'success': False,\n",
    "                'error': 'No backend URL provided'\n",
    "            }), 400\n",
    "\n",
    "        return jsonify({\n",
    "            'success': True,\n",
    "            'message': 'Connection verified',\n",
    "            'timestamp': timestamp\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Verification error: {e}\")\n",
    "        return jsonify({\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }), 500\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def root():\n",
    "    \"\"\"Basic root endpoint for testing\"\"\"\n",
    "    return jsonify({\n",
    "        'status': 'alive',\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "@app.route('/debug', methods=['GET'])\n",
    "def debug():\n",
    "    \"\"\"Debug endpoint with detailed information\"\"\"\n",
    "    return jsonify({\n",
    "        'status': 'debug',\n",
    "        'gpu': {\n",
    "            'available': torch.cuda.is_available(),\n",
    "            'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "            'memory': get_gpu_memory(),\n",
    "            'device_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None\n",
    "        },\n",
    "        'server': {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'pid': os.getpid(),\n",
    "            'python_version': sys.version\n",
    "        }\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Check CUDA availability before starting\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"ERROR: No GPU available. This server requires CUDA GPU support.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Initialize globals\n",
    "    global pipeline, autocast_context\n",
    "    pipeline = None\n",
    "    autocast_context = None\n",
    "    \n",
    "    # Set CUDA device properties\n",
    "    torch.cuda.set_per_process_memory_fraction(0.7)  # Reduced from 0.8 to 0.7\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    import torch._dynamo\n",
    "    torch._dynamo.config.suppress_errors = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    run_server()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
