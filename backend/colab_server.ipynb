{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1hc8G2WY_4P_0Tri-lZ0HmVDdX6MKy5LV?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision diffusers transformers flask flask-cors xformers\n",
    "!npm install -g localtunnel\n",
    "\n",
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "from flask import Flask, request, jsonify, make_response\n",
    "from flask_cors import CORS\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from threading import Thread\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "# Basic Flask setup\n",
    "app = Flask(__name__)\n",
    "CORS(app, resources={\n",
    "    r\"/*\": {\n",
    "        \"origins\": \"*\",\n",
    "        \"methods\": [\"GET\", \"POST\", \"OPTIONS\"],\n",
    "        \"allow_headers\": [\"Content-Type\", \"Authorization\", \"Accept\"],\n",
    "        \"max_age\": 3600\n",
    "    }\n",
    "})\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@app.after_request\n",
    "def after_request(response):\n",
    "    \"\"\"Add CORS headers to all responses\"\"\"\n",
    "    headers = {\n",
    "        'Access-Control-Allow-Origin': '*',\n",
    "        'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',\n",
    "        'Access-Control-Allow-Headers': 'Content-Type, Authorization, Accept',\n",
    "        'Access-Control-Max-Age': '3600',\n",
    "    }\n",
    "    for key, value in headers.items():\n",
    "        response.headers[key] = value\n",
    "    return response\n",
    "\n",
    "# Global variables\n",
    "pipeline = None\n",
    "\n",
    "def setup_pipeline():\n",
    "    pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "        \"stabilityai/sdxl-turbo\",\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    pipeline.enable_xformers_memory_efficient_attention()\n",
    "    return pipeline, None\n",
    "\n",
    "def find_free_port(start_port=5001, max_attempts=10):\n",
    "    import socket\n",
    "    for port in range(start_port, start_port + max_attempts):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('', port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            continue\n",
    "    raise RuntimeError(f\"Could not find free port\")\n",
    "\n",
    "def setup_tunnel(port):\n",
    "    try:\n",
    "        timestamp = datetime.now().strftime('%H%M%S')\n",
    "        subdomain = f\"story-gen-{timestamp}\"\n",
    "        command = f'npx localtunnel --port {port} --subdomain {subdomain}'\n",
    "        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, text=True)\n",
    "        \n",
    "        for _ in range(30):  # 30 seconds timeout\n",
    "            output = process.stdout.readline()\n",
    "            if 'your url is:' in output.lower():\n",
    "                return output.split('is: ')[-1].strip()\n",
    "            time.sleep(1)\n",
    "        raise Exception(\"Tunnel setup timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"Tunnel setup failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@app.route('/generate_image', methods=['POST', 'OPTIONS'])\n",
    "def generate_image():\n",
    "    global pipeline\n",
    "    try:\n",
    "        if request.method == 'OPTIONS':\n",
    "            return jsonify({'success': True})\n",
    "\n",
    "        if pipeline is None:\n",
    "            pipeline, _ = setup_pipeline()\n",
    "            if pipeline is None:\n",
    "                raise Exception(\"Failed to initialize pipeline\")\n",
    "\n",
    "        data = request.get_json()\n",
    "        if not data or 'prompt' not in data:\n",
    "            return jsonify({\n",
    "                'success': False,\n",
    "                'error': 'No prompt provided'\n",
    "            }), 400\n",
    "\n",
    "        prompt = data['prompt']\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            try:\n",
    "                image = pipeline(\n",
    "                    prompt=prompt,\n",
    "                    num_inference_steps=1,\n",
    "                    guidance_scale=0.0,\n",
    "                    width=384,\n",
    "                    height=384\n",
    "                ).images[0]\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    torch.cuda.empty_cache()\n",
    "                    return jsonify({\n",
    "                        'success': False,\n",
    "                        'error': 'GPU out of memory - please try again'\n",
    "                    }), 503\n",
    "                raise\n",
    "\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\", quality=85)\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "        return jsonify({\n",
    "            'success': True,\n",
    "            'image': f'data:image/jpeg;base64,{img_str}'\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        torch.cuda.empty_cache()\n",
    "        return jsonify({\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }), 500\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'gpu_available': torch.cuda.is_available()\n",
    "    })\n",
    "\n",
    "@app.route('/pipeline_status', methods=['GET'])\n",
    "def pipeline_status():\n",
    "    return jsonify({\n",
    "        'initialized': pipeline is not None\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"Initializing SDXL pipeline...\")\n",
    "        pipeline, _ = setup_pipeline()\n",
    "        \n",
    "        port = find_free_port()\n",
    "        server = Thread(target=lambda: app.run(host='0.0.0.0', port=port))\n",
    "        server.daemon = True\n",
    "        server.start()\n",
    "        \n",
    "        tunnel_url = setup_tunnel(port)\n",
    "        print(f\"\\nServer URL: {tunnel_url}\")\n",
    "        print(\"Copy this URL to use in the Story Generator app\")\n",
    "        \n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nShutting down server...\")\n",
    "        os.system('pkill -f lt')\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")\n",
    "        os.system('pkill -f lt')\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
