{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "# Story Generator - Image Server\n",
    "This notebook runs the image generation server using SDXL-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchvision diffusers transformers flask flask-cors pyngrok\n",
    "\n",
    "from google.colab import userdata\n",
    "NGROK_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
    "!ngrok authtoken $NGROK_TOKEN\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "server_code"
   },
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from pyngrok import ngrok\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Initialize the model\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/sdxl-turbo\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'gpu_available': torch.cuda.is_available()\n",
    "    })\n",
    "\n",
    "@app.route('/generate_frame', methods=['POST'])\n",
    "def generate_frame():\n",
    "    try:\n",
    "        data = request.json\n",
    "        prompt = data.get('prompt', '')\n",
    "\n",
    "        if not prompt:\n",
    "            return jsonify({'success': False, 'error': 'No prompt provided'}), 400\n",
    "\n",
    "        # Generate image\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=1,\n",
    "            guidance_scale=0.0,\n",
    "        ).images[0]\n",
    "\n",
    "        # Convert to base64\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "        return jsonify({\n",
    "            'success': True,\n",
    "            'image': f'data:image/jpeg;base64,{img_str}'\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({'success': False, 'error': str(e)}), 500\n",
    "\n",
    "# Start the server\n",
    "ngrok.kill() # Kill any existing tunnels\n",
    "public_url = ngrok.connect(addr=\"5000\", proto=\"http\")\n",
    "print(f'Server running at: {public_url}')\n",
    "\n",
    "app.run(host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
